{"cells":[{"cell_type":"markdown","source":["#### Introduction\n","\n","This set of notebooks (01 - Run in Primary and 02 - Run in DR) demonstrates an automated end-to-end BCDR process to recover supported (Git) items, lakehouses, warehouses and associated data."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c8b103c8-b862-46fc-9783-de05da92a9b6"},{"cell_type":"markdown","source":["##### Prerequisites\n","Please read the disaster recovery guidance found in <a href=\"https://learn.microsoft.com/en-us/fabric/security/experience-specific-guidance\">the documentation</a> to obtain a general understanding of how to recover lakehouse and warehouse data.\n","When the OneLake DR setting has been enabled at capacity level, lakehouse and warehouse data will be geo-replicated to the secondary (paired) region. As mentioned in the documentation, this data may be inaccessible through the normal Fabric UI experience, therefore the data will need to be recovered into a corresponding (new) workspace in the DR region using the storage endpoints (abfs paths) as depicted in the image below. \n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/reco_from_docs.png?raw=true\" width=\"800\"/>\n","</div>\n","\n","To use this notebook ensure this runs in a new workspace (Reco1) attached to a new lakehouse (LH_BCDR) and ensure DR is enabled for the assocated capacity (C1).\n","\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/before_recovery.png?raw=true\" width=\"800\"/>\n","</div>\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"23fc709e-0e70-4375-8b8c-a6e1218a2148"},{"cell_type":"markdown","source":["###### About this notebook\n","\n","This part of the BCDR process collects metadata about your Fabric environment, such as information about your  capacities, workspaces, their assocated Git connections (if connected to Git) and Fabric items. This metadata is used in second notebook \"02 - Run in DR\" during the recovery process. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95d86d1e-330c-455a-95ed-e67aadcca51e"},{"cell_type":"markdown","source":["##### Notebook parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"616c5d3a-b1c1-4588-8a23-06baeb4a2925"},{"cell_type":"code","source":["# Specify workspaces to ignore in this part of the process which collects metadata about your Fabric environment. \n","# Either use exact workspace names or prefix or suffix with % as wildcard \n","# This is useful when you have have many workspaces which don't need to be recovered \n","# for businesss continuity purposes and collecting required metadata would take an unecessary amount of time \n","# e.g. to ignore any workspaces suffixed with DEV or TEST use ['%DEV','%TEST%'] \n","\n","# These list parameters need to be in the format of ['string1','string2',...'stringN']. Use [] for an empty list.\n","\n","# Specify an exact list of workspaces to ignore e.g. p_ws_ignore_list = ['Microsoft Fabric Capacity Metrics 26/02/2024 16:15:42','AdminInsights']\n","p_ws_ignore_list = [] \n","# Specify a list with wildcards using % e.g. to ignore anything with _DEV and _TEST as a suffix p_ws_ignore_like_list = ['%_DEV%','%_TEST%']  \n","p_ws_ignore_like_list = []\n","\n","# Boolean parameter to specify verbose informational messages. \n","# Only set to True if additional logging information required, otherwise notebook may generate significant (print) messages.\n","p_logging_verbose = False"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"f7308100-7d02-4698-ae2b-ab8dcfe30433"},{"cell_type":"markdown","source":["##### Imports and Utility Functions\n","Ensure the cell below is runs successfully to include the supporting functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dee81614-b92b-4242-890a-b11f97b1a640"},{"cell_type":"code","source":["%run workspaceutils"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"391624c1-b299-452d-9ebf-f32626d49970"},{"cell_type":"markdown","source":["##### Check for default lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c5968174-5758-45ec-8c44-e3e1b7a3f42f"},{"cell_type":"code","source":["if (notebookutils.runtime.context['defaultLakehouseId']==None):\n","    displayHTML('<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h4>Please set a default lakehouse before proceeding</span><img style=\"float: right; margin-left: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"></div>')\n","    print('\\n')\n","    raise noDefaultLakehouseException('No default lakehouse found. Please add a lakehouse to this notebook.')\n","else: \n","    print('Default lakehouse is set to '+ notebookutils.runtime.context['defaultLakehouseName'] + '('+ notebookutils.runtime.context['defaultLakehouseId'] + ')')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1476aa8b-a1e6-41ea-9ebc-d2ee37f8f165"},{"cell_type":"markdown","source":["##### Store metadata of Capacities, Workspaces and Items"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db34d1b1-b30e-45d9-80f4-00ac9c77e88d"},{"cell_type":"code","source":["print('Gathering recovery metadata...')\n","saveCapacityMeta()\n","saveWorkspaceMeta()\n","saveItemMeta(verbose_logging=p_logging_verbose, ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list)\n","saveReportMeta(verbose_logging=p_logging_verbose,only_secondary=False,ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list)\n","print('Done')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd025963-2232-400e-a30e-53d8e0a821ac"},{"cell_type":"markdown","source":["##### Save Workspace Git Connection Details"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c25c1e21-e58b-4d28-b748-3594cc823ce9"},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n","\n","table_name = 'gitconnections'\n","spark.sql(\"Drop table if exists \"+ table_name)\n","wsgitconnsql  =\"SELECT distinct ID,Type,Name FROM workspaces where Type!='AdminInsights'\"\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        wsgitconnsql  = wsgitconnsql + \" and name not like '\" + notlike + \"'\"\n","if len(p_ws_ignore_list)>0:\n","    wsgitconnsql  = wsgitconnsql + \" and name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","#print(wsgitconnsql)\n","\n","dfwks = spark.sql(wsgitconnsql).collect()\n","print(\"Retreiving Git details for any connected workspaces. This may take a few minutes...\")\n","for idx,i in enumerate(dfwks):\n","    if i['Type'] == 'Workspace':\n","        url = \"/v1/workspaces/\" + i['ID'] + \"/git/connection\"\n","        try:\n","            if p_logging_verbose:\n","                print(\"Storing git details for workspace \"+i['Name']+\" (\"+i['ID']+')...')\n","            response = client.get(url)\n","\n","            gitProviderDetailsJSON = response.json()['gitProviderDetails']\n","            gitConnectionStateJSON = response.json()['gitConnectionState']\n","            gitSyncDetailsJSON = response.json()['gitSyncDetails']\n","            df = spark.createDataFrame([i['ID']],\"string\").toDF(\"Workspace_ID\")\n","            df=df.withColumn(\"Workspace_Name\",lit(i['Name'])).withColumn(\"gitConnectionState\",lit(gitConnectionStateJSON)).withColumn(\"gitProviderDetails\",lit(json.dumps(gitProviderDetailsJSON))).withColumn(\"gitSyncDetails\",lit(json.dumps(gitSyncDetailsJSON)))\n","\n","            if idx == 0:\n","                dfall = df\n","            else:\n","                dfall= dfall.union(df)\n","        except Exception as error:\n","            errmsg =  \"Couldn't get git connection status for workspace \" + i['Name'] + \"(\"+ i['ID'] + \").\"\n","            errmsg = errmsg + \"Error: \"+str(error)\n","            print(str(errmsg))\n","\n","dfall.withColumn(\"metaTimestamp\",current_timestamp()).write.mode('overwrite').option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n","print('Done')\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8eebcaa-1262-4ecd-b914-3ea6665f9908"},{"cell_type":"markdown","source":["##### Save Workspace Role Assignments"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19198848-3fe5-46f5-a9eb-c7628923e4cd"},{"cell_type":"code","source":["# iterate through workspaces and store role assignments in a list\n","import ast\n","all_role_data = []\n","table_name = 'wsroleassignments'\n","\n","spark.sql(\"Drop table if exists \"+ table_name)\n","\n","rolesql = \"SELECT distinct ID,Type,Name FROM workspaces where Type!='AdminInsights'\"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        rolesql  = rolesql + \" and name not like '\" + notlike + \"'\"\n","if len(p_ws_ignore_list)>0:\n","    rolesql  = rolesql + \" and name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","print(\"Retreiving workspace role assignments...\")\n","\n","roledf = spark.sql(rolesql).collect()\n","for idx,i in enumerate(roledf):\n","    if i['Type'] == 'Workspace':\n","        url = \"/v1/workspaces/\" +i['ID'] + \"/roleAssignments\"\n","        try:\n","            if p_logging_verbose:\n","                print('Retreiving roles asignments for workspace '+ i['Name'] + '...')\n","            raresponse = client.get(url)\n","            roleassignments  =  raresponse.json()['value']\n","            for roleassignment in  roleassignments:\n","                roleassignment['workspaceName']=i['Name']\n","                roleassignment['workspaceId']=i['ID']\n","            #print(roleassignments)\n","            all_role_data.extend(roleassignments)\n","            \n","        except Exception as error:\n","            errmsg =  \"Couldn't retreive role assignments for workspace \" + i['Name'] + \"(\"+ i['ID'] + \"). Error: \"+str(error)\n","            print(str(errmsg))\n","\n","if all_role_data is not None and len(all_role_data)>0:\n","    roleassignmentdf = spark.read.json(sc.parallelize(all_role_data)) \\\n","        .withColumn('principalId',col('principal')['id']) \\\n","        .withColumn('displayName',col('principal')['displayName']) \\\n","        .withColumn('principalType',col('principal')['type']) \\\n","        .withColumn('userPrincipalName',col('principal')['userDetails']['userPrincipalName']) \\\n","        .withColumn('workspaceId',col('workspaceId')) \\\n","        .withColumn('workspaceName',col('workspaceName')) \\\n","        .drop('principal')\n","    #display(roleassignmentdf)\n","    print(saveTable(roleassignmentdf,'wsroleassignments') ) \n","else:\n","    print('No role data found.')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d1ce897f-7633-4237-9997-c6251abd0c8d"},{"cell_type":"markdown","source":["##### Complete. \n","\n","Now proceed to Stage 3 in the associated accelerator documentation."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4d270ea0-a313-4964-b362-24812d500bc2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}