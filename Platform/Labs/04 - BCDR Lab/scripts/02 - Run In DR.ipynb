{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":{"default_lakehouse":null,"default_lakehouse_name":"","default_lakehouse_workspace_id":""},"environment":{}}},"cells":[{"cell_type":"markdown","metadata":null,"source":["##### Prerequisites\n","Please read the disaster recovery guidance found in <a href=\"https://learn.microsoft.com/en-us/fabric/security/experience-specific-guidance\">the documentation</a> to obtain a general understanding of how to recover lakehouse and warehouse data.\n","When the OneLake DR setting has been enabled at capacity level, lakehouse and warehouse data will be geo-replicated to the secondary (paired) region - which may not be accessible through the normal Fabric UI experience. Therefore, in a DR scenario, this data will need to be recovered into a corresponding (new) workspace in the DR region using the storage endpoints (abfs paths) as depicted in the image below. \n","<!--<div style=\"margin: 0 auto; text-align: center; overflow: hidden;\">\n","<div style=\"float: left;\"> -->\n","<img src=\"https://github.com/hurtn/images/blob/main/reco_from_docs.png?raw=true\" width=\"800\"/>\n","<!--<small><b>Figure 1</b></small></div></div><br> -->\n","\n","To use this recovery notebook please ensure:<p>&nbsp;</p>\n","1. You have imported and run the \"01 - Run in Primary\" notebook in the primary region.\n","<!-- <div style=\"margin: 0 auto; text-align: center; overflow: hidden;\">\n","<div style=\"float: left;\"> -->\n","<img src=\"https://github.com/hurtn/images/blob/main/before_recovery.png?raw=true\" width=\"800\"/>\n","<!-- <small><b>Figure 2</b></small></div></div><br> --><p>&nbsp;</p>\n","2. There is at least one capacity (C2) created in the DR region. \n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage1v2.png?raw=true\" width=\"800\"/>\n","<!--<small><b>Figure 3</b></small> </div></div><br> --><p>&nbsp;</p>\n","3. A recovery workspace (Reco2) has been created in the secondary region (C2.Reco2) which contains this notebook \"02 - Run in DR\" and attached to a default lakehouse (C2.Reco2.LH_BCDR).\n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage2v2.png?raw=true\" width=\"800\"/>\n","<!-- <small><b>Figure 4</b></small> </div></div> -->\n","<p>&nbsp;</p>\n","4. Set the values for the notebook parameters below according to your item names. Sample names are shown below if using the naming scheme in the image above. Note: If passing these parameters in from a pipeline these defaults will be overridden:<p>&nbsp;</p>\n","<left>\n","p_bcdr_workspace_src -> Reco1 <br>\n","p_bcdr_workspace_tgt -> Reco2  <br>\n","p_bcdr_lakehouse_src -> LH_BCDR <br>\n","p_bcdr_lakehouse_tgt -> LH_BCDR <br>\n","p_secondary_ws_suffix eg '_SECONDARY' - specify a suffix which will be appended to each workspace created in the DR region. This is to ensure uniqueness of workspace names and is useful for identifying these worksspaces when testing in a non-DR scenario also.  <br>\n","p_recovered_object_suffix eg '_recovered'- specify a suffix that will be appended to each table name created in the recovery lakehouse C2.Reco2.LH_BCDR <br>\n","list_of_workspaces_to_recover eg ['WS1','WS2'] - specify a list of workspaces to specifically recover. This is useful for testing in a non-DR scenario also. Leave empty [''] for all workspaces.  <br>"]},{"cell_type":"code","metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Specify the source and target BCDR workspaces and lakehouses\n","p_bcdr_workspace_src = ''\n","p_bcdr_workspace_tgt = ''\n","p_bcdr_lakehouse_src = ''\n","p_bcdr_lakehouse_tgt = ''\n","\n","# Specify the DR capacity ID. To get the list of IDs simply run fabric.list_capacities() in a new cell below the loading the utility functions\n","p_dr_capacity_id = ''\n","\n","# This variable adds a suffix to the name of the new workspaces created to ensure there are no naming conflicts with the original workspace name. \n","# Ensure that you use a string that will gaurantee uniqueness rather than common terms which may be used by others in day to day activities.  \n","p_secondary_ws_suffix = '_SECONDARY'\n","p_recovered_object_suffix = '_recovered'\n","\n","# Determines whether to add role assignments to the new workspaces. If you prefer to apply these at a later stage set the value to False. \n","p_add_ws_role_assignments = True\n","\n","# List parameters below need to be in the format of ['string1','string2',...'stringn']. Empty lists must be declared as []\n","# Specify the list of workspaces to recover, leave empty [] to recover all. For specific workspaces e.g. p_list_of_workspaces_to_recover = ['Workspace1','Workspace2'] \n","p_list_of_workspaces_to_recover = []\n","# Specify an exact list of workspaces to ignore e.g. p_ws_ignore_list = ['Microsoft Fabric Capacity Metrics 26/02/2024 16:15:42','AdminInsights']\n","p_ws_ignore_list = [] # add workspaces to this list to ignore them from the metadata extract process including git details\n","# Specify a list with wildcards using % e.g. to ignore anything with _DEV and _TEST as a suffix p_ws_ignore_like_list = ['%_DEV%','%_TEST%']  \n","p_ws_ignore_like_list = []\n","\n","# Boolean parameter to specify verbose informational messages. \n","# Only set to True if additional logging information required, otherwise notebook may generate significant (print) messages.\n","p_logging_verbose = False"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Imports and Utility Functions\n","Ensure the cell below is runs successfully to include all the helper utilities"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["%run workspaceutils"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["###### Check default lakehouse"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["if (notebookutils.runtime.context['defaultLakehouseId']==None):\n","    displayHTML('<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h4>Please set a default lakehouse before proceeding</span><img style=\"float: right; margin-left: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"></div>')\n","    print('\\n')\n","    raise noDefaultLakehouseException('No default lakehouse found. Please add a lakehouse to this notebook.')\n","else: \n","    print('Default lakehouse is set to '+ notebookutils.runtime.context['defaultLakehouseName'] + '('+ notebookutils.runtime.context['defaultLakehouseId'] + ')')"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 4: Recover metadata tables from the BCDR workspace\n","\n","In order for the recovery process to begin, it needs the metadata of the primary environment (created by running the Store DR Metadata notebook) such as workspaces. This data will be persisted as tables in the default lakehouse of this notebook.\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/starting_reco_stage2v3.png?raw=true\" width=\"800\"/>\n","</div>\n"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Gathers the list of recovers tables and source paths to be copied into the lakehouse associated with this notebook \n","\n","src_path = f'abfss://{p_bcdr_workspace_src}@onelake.dfs.fabric.microsoft.com/{p_bcdr_lakehouse_src}.Lakehouse'\n","\n","table_list = get_lh_object_list(src_path)\n","print('The following tables will attempt to be recovered and persisted as tables in the default lakehouse of this notebook...')\n","display(table_list)"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["print('Restoring recovery tables...')\n","res = copy_lh_objects(table_list[table_list['type']=='table'],p_bcdr_workspace_src,p_bcdr_workspace_tgt,\n","                      p_bcdr_lakehouse_src,p_bcdr_lakehouse_tgt,p_recovered_object_suffix,False)\n","print('Done.')\n","display(res)\n"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 5: Recreate workspaces\n","Recover the workspaces that used to exist in the primary. The suffix parameter will be appended to the workspace name\n"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["recovered_ws_sql = \"SELECT distinct ID,Type,Name,Capacity_Id FROM workspaces\" + p_recovered_object_suffix + \" where 1=1 \"\n","\n","if len(p_list_of_workspaces_to_recover)>0:\n","  recovered_ws_sql = recovered_ws_sql+\" and Name in ('\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","if len(p_ws_ignore_list)>0:\n","   recovered_ws_sql = recovered_ws_sql+ \" and Name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        recovered_ws_sql  = recovered_ws_sql + \" and name not like '\" + notlike + \"'\"\n","\n","print('Recreating workspaces with suffix of '+ p_secondary_ws_suffix + '...')\n","#print(recovered_ws_sql)\n","df = spark.sql(recovered_ws_sql).collect()\n","for i in df:\n","    #print(i['ID'])\n","    if i['Type'] == 'Workspace':\n","      try:\n","        if p_logging_verbose:\n","          print(\"Creating workspace: \" + i['Name']+p_secondary_ws_suffix + \" in capacity \"+ i['Capacity_Id']+\"...\")\n","        response = fabric.create_workspace(i['Name']+p_secondary_ws_suffix,p_dr_capacity_id)\n","        if p_logging_verbose:\n","          print(\"Created workspace with ID: \" + response)\n","      except Exception as error:\n","          errmsg =  \"Failed to recreate workspace \" + i['Name'] +p_secondary_ws_suffix + \" with capacity ID (\"+ i['Capacity_Id'] + \") due to: \"+str(error)\n","          print(errmsg)\n","print('Now reloading workspace metadata table...')\n","# Now popupate the workspace metadata table\n","saveWorkspaceMeta()\n","print('Done.')\n"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["#### Stage 6: Connect Git and Initialise"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["gitsql = \"select gt.gitConnectionState,gt.gitProviderDetails, wks.name Workspace_Name, wks.id Workspace_ID from gitconnections_recovered gt \" \\\n","         \"inner join workspaces wks on gt.Workspace_Name = replace(wks.name,'\" + p_secondary_ws_suffix+ \"','') \" \\\n","         \"where gt.gitConnectionState = 'ConnectedAndInitialized' and wks.name like '%\"+p_secondary_ws_suffix+\"' and wks.id != '\"+thisWsId+\"'\" \n","\n","if len(p_list_of_workspaces_to_recover)>0:\n","    gitsql = gitsql+\" and gt.Workspace_Name in ('\"\"\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","if len(p_ws_ignore_list)>0:\n","    gitsql = gitsql+\" and gt.Workspace_Name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        gitsql  = gitsql + \" and name not like '\" + notlike + \"'\"\n","\n","print('Reconnecting workspaces to Git...')\n","df = spark.sql(gitsql).collect()\n","\n","for idx,i in enumerate(df):\n","    if i['gitConnectionState'] == 'ConnectedAndInitialized':\n","        \n","        url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/connect\"\n","        payload = '{\"gitProviderDetails\": ' + i['gitProviderDetails'] + '}'\n","        #print(str(payload))\n","\n","        try:\n","            if p_logging_verbose:\n","                print('Attempting to connect workspace '+ i['Workspace_Name'])\n","            response = client.post(url,json= json.loads(payload))\n","            if p_logging_verbose:\n","                print(str(response.status_code) + response.text) \n","            success = True\n","            \n","        except Exception as error:\n","            error_string = str(error)\n","            error_index = error_string.find(\"Error:\")\n","\n","            # Extract the substring after \"Error:\"\n","            error_message = error_string[error_index + len(\"Error:\"):].strip()\n","            headers_index = error_message.find(\"Headers:\")\n","\n","            # Extract the substring before \"Headers:\"\n","            error_message = error_message[:headers_index].strip()\n","            error_data = json.loads(error_message)\n","            # Extract the error message\n","            error_message = error_data.get(\"message\", \"\")\n","\n","            errmsg =  \"Couldn't connect git to workspace \" + i['Workspace_Name'] + \"(\"+ i['Workspace_ID'] + \"). Error: \"+str(error_message)\n","            print(str(errmsg))\n","            success = False\n","        # If connection successful then try to initialise    \n","        if (success):\n","            url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/initializeConnection\"\n","            payload = {\"initializationStrategy\":\"PreferRemote\"}\n","            try:\n","                if p_logging_verbose:\n","                    print('Attempting to initialize git connection for workspace '+ i['Workspace_Name'])\n","                response = client.post(url,json= payload)\n","                #print(str(response.status_code) + response.text) \n","                commithash = response.json()['remoteCommitHash']\n","                if p_logging_verbose:\n","                    print('Successfully initialized. Updating with commithash '+commithash)\n","                if commithash!='':\n","                    url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/updateFromGit\"\n","                    payload = '{\"remoteCommitHash\": \"' + commithash + '\",\"conflictResolution\": {\"conflictResolutionType\": \"Workspace\",\"conflictResolutionPolicy\": \"PreferWorkspace\"},\"options\": {\"allowOverrideItems\": true}}'\n","                    response = client.post(url,json= json.loads(payload))\n","                    if p_logging_verbose and response.status_code==200:\n","                        print('Successfully started sync')\n","            except Exception as error:\n","                success = False\n","                error_string = str(error)\n","                error_index = error_string.find(\"Error:\")\n","\n","                # Extract the substring after \"Error:\"\n","                error_message = error_string[error_index + len(\"Error:\"):].strip()\n","                headers_index = error_message.find(\"Headers:\")\n","\n","                # Extract the substring before \"Headers:\"\n","                error_message = error_message[:headers_index].strip()\n","                error_data = json.loads(error_message)\n","                # Extract the error message\n","                error_message = error_data.get(\"message\", \"\")\n","                errmsg =  \"Couldn't initialize git for workspace \" + i['Workspace_Name'] + \"(\"+ i['Workspace_ID'] + \"). Error: \"+str(error_message)\n","                print(str(errmsg))\n","\n","                        \n","if success:        \n","    print('Done')\n","else:\n","    print('Completed with errors.')"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h5>Only run the cell below if you wish to disconnect the workspaces from git and re-run the cell above</span></div>\n","Note: Using Run all will not run this cell as it should be frozen\n"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# disconnect\n","\n","for idx,i in enumerate(df):\n","    if i['gitConnectionState'] == 'ConnectedAndInitialized':\n","\n","            url = \"/v1/workspaces/\" + i['Workspace_ID'] + \"/git/disconnect\"\n","            response = client.post(url)\n","\n"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### After git sync completes then capture environment metadata (items and reports)  \n","#TODO: Need to write a method to check the sync is complete before attempting to extract the items. For now you will need to wait until the sync has complete and all workspaces have been updated from git. For now adding a sleep into the process to make sure it waits a minute for the sync to complete."]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["import time\n","# temoparary wait while items sync from git. In future this code should be replaced to check for complete status using LRO before proceeding\n","time.sleep(60)\n","print('Gathering metadata about reports and items... ')\n","\n","saveItemMeta(verbose_logging=p_logging_verbose, ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list)\n","saveReportMeta(verbose_logging=p_logging_verbose,only_secondary=True,ws_ignore_list=p_ws_ignore_list,ws_ignore_like_list=p_ws_ignore_like_list)"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 7: Recover lakehouse data (files and tables) to corresponding recovered workspaces\n","\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/lh_reocvery.png?raw=true\" width=\"800\"/>\n","</div>"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["thisWsId = notebookutils.runtime.context['currentWorkspaceId'] #failsafe: obtaining this workspace id so we don't accidently overwrite objects in this workspace!\n","\n","data_recovery_sql = \"\"\"select wr.Name primary_ws_name, ir.type primary_type, ir.DisplayName primary_name,  \n","wr.id primary_ws_id, ir.id primary_id,wks.id secondary_ws_id, wks.Name secondary_ws_name, it.id secondary_id, \n","it.DisplayName secondary_name, cr.display_name capacity_name\n","from items_recovered ir \n","    inner join workspaces_recovered wr on wr.Id = ir.WorkspaceId \n","    inner join capacities_recovered cr on wr.capacity_id = cr.id\n","    inner join workspaces wks on wr.Name = replace(wks.name,'\"\"\" + p_secondary_ws_suffix+ \"\"\"','')\n","    inner join items it on ir.DisplayName = it.DisplayName and it.WorkspaceId = wks.Id \n","where ir.type in ('Lakehouse','Warehouse') and it.type = ir.type and wr.Name in ('\"\"\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"\"\"') \n","     and wks.name like '%\"\"\"+p_secondary_ws_suffix+\"\"\"'  and wks.id != '\"\"\"+thisWsId+\"\"\"' order by ir.type,primary_ws_name\"\"\"\n","#print(data_recovery_sql)\n","print(\"The subsequent notebook cells attempt to recover lakehouse and warehouse data for the following items...\")\n","df_recovery_items=spark.sql(data_recovery_sql)\n","\n","# populate dataframes for lakehouse metadata and warehouse metadata respectively \n","lakehousedf = df_recovery_items.filter(\"primary_type='Lakehouse'\").collect()\n","warehousedf = df_recovery_items.filter(\"primary_type='Warehouse'\").collect()\n","\n","display(df_recovery_items.select(\"primary_ws_name\",\"primary_type\",\"primary_name\",\"secondary_ws_name\",\"secondary_name\", \"secondary_ws_id\", \"capacity_name\"))\n","\n"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["print('The following lakehouse(s) will attempt to be recovered... ')\n","display(lakehousedf)\n","\n","for idx,i in enumerate(lakehousedf):\n","    # Define the full abfss source path of the primary BCDR workspace and lakehouse \n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    if p_logging_verbose:\n","        print('Attempting to recover items for workspace: ' + i['primary_ws_name'] + ', lakehouse: ' + i['primary_name'] + ' into target workspace ' + i['secondary_ws_name'] + ' lakehouse ' + i['secondary_name'])\n","    table_list = get_lh_object_list(src_path)\n","\n","    #recover files\n","    copy_lh_objects(table_list[table_list['type']=='file'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","    #recover tables\n","    copy_lh_objects(table_list[table_list['type']=='table'],i['primary_ws_id'], i['secondary_ws_id'] ,i['primary_id'],i['secondary_id'],p_recovered_object_suffix,False,True)\n","\n","print('Done')"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 8: Prepare warehouse recovery\n","###### Important: \n","This process creates a staging lakehouse to store shortcuts which point back to the DR copy of the tables (actually delta folders) - in testing this will point back to folders in the primary region but in a true DR scenario where Microsoft has failed over the OneLake endpoints, they will point back to folders in the secondary (paired) storage region.\n","A parameterised pipeline is injected into each target workspace which will load target tables from these shortcuts.\n","This staging lakehouse and pipeline can be deleted manually after the datawarehouse tables have been successfully recovered.\n","<div>\n","<img src=\"https://github.com/hurtn/images/blob/main/wh_recovery.png?raw=true\" width=\"1000\"/>\n","</div>"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["print('The following warehouse(s) will attempt to be recovered... ')\n","display(warehousedf)\n","print('\\nPreparing for recovery...\\n')\n","# interate through all the data warehouses to recover\n","for idx,i in enumerate(warehousedf):\n","    if p_logging_verbose:\n","        print('Setting up for recovery of warehouse '+i['primary_ws_name'] + '.'+i['primary_name'] + ' into ' + i['secondary_ws_name'] + '.'+i['secondary_name'] )\n","\n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    tgt_path = f'abfss://'+i['secondary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['secondary_id']\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    #display(schema_list)\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'].to_list(),src_path)\n","    #display(table_list)\n","  \n","    # create a temporary staging lakehouse per warehouse to create shortcuts into, \n","    # which point back to original warehouse data currently in the DR storage account\n","    lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name']\n","    # check if it exists before attempting create\n","    if p_logging_verbose:\n","        print('Checking whether the temporary lakehouse \"'+ lhname +'\" exists in workspace '+i['secondary_ws_name']+'...')\n","    temp_lh_id = getItemId(i['secondary_ws_id'],lhname,'Lakehouse')\n","    if temp_lh_id == 'NotExists':\n","        lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name'][:256] # lakehouse name should not exceed 256 characters\n","        payload = payload = '{\"displayName\": \"' + lhname + '\",' \\\n","        + '\"description\":  \"Interim staging lakehouse for primary warehouse recovery: ' \\\n","        + i['primary_ws_name']+'_'+i['primary_name'] + 'into workspace '+ i['secondary_ws_name'] + '(' + i['secondary_ws_id'] +')\"}'\n","        try:\n","            lhurl = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/lakehouses\"\n","            lhresponse = client.post(lhurl,json= json.loads(payload))\n","            temp_lh_id = lhresponse.json()['id']\n","            if p_logging_verbose:\n","                print('Temporary lakehouse \"'+ lhname +'\" created with Id ' + temp_lh_id + ': ' + str(lhresponse.status_code) + ' ' + str(lhresponse.text))\n","        except Exception as error:\n","            print(error.errorCode)\n","    else:\n","        if p_logging_verbose:\n","            print('Temporary lakehouse '+lhname+' (' + temp_lh_id + ') already exists.')\n","        \n","\n","    # Create shortcuts for every table in the format of schema_table under the tables folder\n","    for index,itable in table_list.iterrows():\n","        #print(itable)\n","        #url = \"/v1/workspaces/\" + i['secondary_ws_id'] + \"/items/\"+ i['secondary_id'] +\"/shortcuts\"\n","        #df94e950-e4e3-4b96-86c3-154b884c6a31\n","        shortcutExists=False\n","        # Check if shortcut exists\n","        try:\n","            url = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/items/\" + temp_lh_id + \"/shortcuts/Tables/\"+itable['schema']+'_'+itable['name']\n","            tlhresponse = client.get(url)\n","            shortcutExists = True\n","            if p_logging_verbose:\n","                print('Shortcut '+itable['schema']+'_'+itable['name'] +' already exists')\n","        except Exception as error:\n","            shortcutExists = False    \n","\n","        if not shortcutExists: \n","            # Create shortcuts - one per table per schema\n","            url = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/items/\" + temp_lh_id + \"/shortcuts\"\n","            scpayload = '{' \\\n","            '\"path\": \"Tables/\",' \\\n","            '\"name\": \"'+itable['schema']+'_'+itable['name']+'\",' \\\n","            '\"target\": {' \\\n","            '\"oneLake\": {' \\\n","                '\"workspaceId\": \"' + i['primary_ws_id'] + '\",' \\\n","                '\"itemId\": \"'+ i['primary_id'] +'\",' \\\n","                '\"path\": \"/Tables/' + itable['schema']+'/'+itable['name'] + '\"' \\\n","                '}}}' \n","            try:\n","                #print(scpayload)                \n","                shctresponse = client.post(url,json= json.loads(scpayload))\n","                if p_logging_verbose:\n","                    print('Shortcut '+itable['schema']+'_'+itable['name'] + ' created.' )\n","\n","            except Exception as error:\n","                print('Error creating shortcut '+itable['schema']+'_'+itable['name']+' due to '+str(error) + ':' + shctresponse.text)\n","    \n","    recovery_pipeline_prefix= 'plRecover_Warehouse'       \n","    # recovery pipeline name should not exceed 256 characters\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'][:256]\n","    if p_logging_verbose:\n","        print('Attempting to deploy a copy pipeline in the target workspace to load the target warehouse tables from the shortcuts created above... ')\n","    # Create the pipeline in the target workspace that loads the target warehouse from shortcuts created above \n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    #print(plid)\n","    if plid == 'NotExists':\n","      plid = createDWrecoverypl(i['secondary_ws_id'],recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'])\n","      if p_logging_verbose:\n","          print('Recovery pipeline ' + recovery_pipeline + ' created with Id '+plid)\n","    else:\n","      if p_logging_verbose:\n","          print('Datawarehouse recovery pipeline \"' + recovery_pipeline + '\" ('+plid+') already exist in workspace \"'+i['secondary_ws_name'] + '\" ('+i['secondary_ws_id']+')')  \n","          print('\\n')\n","print('Done')     \n","\n"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 9: Recover warehouse data by running copy pipelines"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["bearerToken = notebookutils.credentials.getToken('pbi')\n","headers = {\n","\"Authorization\": f\"Bearer {bearerToken}\", \n","\"Content-Type\": \"application/json\"  # Set the content type based on your request\n","}\n","print('Starting warehouse recovery pipelines...')\n","# interate through all the data warehouses to recover\n","for idx,i in enumerate(warehousedf):\n","    if p_logging_verbose:\n","        print('Invoking pipeline to copy warehouse data from  '+i['primary_ws_name'] + '.'+i['primary_name'] + ' into ' + i['secondary_ws_name'] + '.'+i['secondary_name'] )\n","\n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    #tgt_path = f'abfss://'+i['secondary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['secondary_id']\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    #display(schema_list)\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'].to_list(),src_path)\n","\n","    tablesToCopyParam = table_list[['schema','name']].to_json( orient='records')\n","    # ensure the temporary lakehouse exists\n","    lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name']\n","    temp_lh_id = getItemId(i['secondary_ws_id'],lhname,'Lakehouse')\n","    \n","    # obtain the connection string for the warehouse to pass to the copy pipeline\n","    whurl  = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/warehouses/\" + i['secondary_id']\n","    whresponse = client.get(whurl)\n","    connStr = whresponse.json()['properties']['connectionInfo']\n","\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'][:256]\n","    # obtain the pipeline id created to recover this warehouse\n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    if plid == 'NotExists':\n","        print('Error: Could not execute pipeline '+recovery_pipeline+ ' as the ID could not be obtained ')\n","    else:\n","        # pipeline url including pipeline Id unique to each warehouse\n","        plurl = 'v1/workspaces/'+i['secondary_ws_id'] +'/items/'+plid+'/jobs/instances?jobType=Pipeline'\n","        #print(plurl)\n","\n","        payload_data = '{' \\\n","            '\"executionData\": {' \\\n","                '\"parameters\": {' \\\n","                    '\"lakehouseId\": \"' + temp_lh_id + '\",' \\\n","                    '\"tablesToCopy\": ' + tablesToCopyParam + ',' \\\n","                    '\"workspaceId\": \"' + i['secondary_ws_id'] +'\",' \\\n","                    '\"warehouseId\": \"' + i['secondary_id'] + '\",' \\\n","                    '\"connStr\": \"' + connStr + '\"' \\\n","                    '}}}'\n","        #print(payload_data)\n","        plresponse = client.post(plurl, json=json.loads(payload_data), headers=headers)\n","        if p_logging_verbose:\n","            print(str(plresponse.status_code))      \n","print('Done')"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Stage 10: Add workspace roles assignments to the new workspaces"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["if p_add_ws_role_assignments:\n","    ws_role_sql = \"SELECT wks.ID new_workspace_id, wks.name new_workspace, rar.* FROM wsroleassignments_recovered rar inner join workspaces wks on rar.workspaceName = replace(wks.Name,'\" + p_secondary_ws_suffix+ \"','')\" \\\n","                \"where wks.name like '%\"+p_secondary_ws_suffix+\"' and wks.id != '\"+thisWsId+\"'\" \n","\n","    # Only apply roles to the (new) workspaces based the list of workspaces defined in the parameter section at the top of this notebook. \n","    # Note that the list is based on the workspace name defined in the primary but will be translated to the associated (new) workspace recently created in the secondary.\n","    if len(p_list_of_workspaces_to_recover)>0:\n","        ws_role_sql = ws_role_sql+\" and rar.workspaceName in ('\" +  \"', '\".join(p_list_of_workspaces_to_recover)+ \"') \"\n","\n","    # Ingore workspaces based on the ignore list defined in the parameter section at the top of this notebook\n","    if len(p_ws_ignore_list)>0:\n","        ws_role_sql = ws_role_sql+ \" and rar.workspaceName not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","\n","    if len(p_ws_ignore_like_list)>0:\n","        for notlike in p_ws_ignore_like_list:\n","            ws_role_sql  = ws_role_sql + \" and name not like '\" + notlike + \"'\"\n","    \n","    print('Adding workspace role assignments...')\n","\n","    #print(ws_role_sql)\n","    dfwsrole = spark.sql(ws_role_sql).collect()\n","    for idx,i in enumerate(dfwsrole):\n","        wsroleurl = \"/v1/workspaces/\" + i['new_workspace_id'] + \"/roleAssignments\"\n","        wsrolepayload = '{\"principal\": {\"id\": \"'+i['principalId']+'\", \"type\": \"'+i['principalType']+'\"},\"role\": \"'+i['role']+'\"}'\n","        #print(str(wsrolepayload))\n","        \n","        try:\n","            if p_logging_verbose:\n","                print(\"Attempting to add role assignments \" + i['role'] + \" for \" +  i['principalType'] + \" princpal \" +i['displayName'] + \" (\" +i['principalId'] + \") to workspace \" + i['new_workspace'] + \"(\"+ i['new_workspace_id'] + \")...\")\n","\n","            response = client.post(wsroleurl,json= json.loads(wsrolepayload))\n","\n","            success = True\n","        except Exception as error:\n","            error_string = str(error)\n","            error_index = error_string.find(\"Error:\")\n","\n","            # Extract the substring after \"Error:\"\n","            error_message = error_string[error_index + len(\"Error:\"):].strip()\n","            headers_index = error_message.find(\"Headers:\")\n","\n","            # Extract the substring before \"Headers:\"\n","            error_message = error_message[:headers_index].strip()\n","            error_data = json.loads(error_message)\n","            # Extract the error message\n","            error_message = error_data.get(\"message\", \"\")\n","            if error_message is not None:\n","                errmsg =  \"Couldn't add role assignment \" + i['role'] + \" for princpal \" +i['displayName'] + \" to workspace \" + i['workspaceName'] + \"(\"+ i['workspaceId'] + \"). Error: \"+error_message\n","            else:\n","                errmsg =  \"Couldn't add role assignment \" + i['role'] + \" for princpal \" +i['displayName'] + \" to workspace \" + i['workspaceName'] + \"(\"+ i['workspaceId'] + \").\"\n","            print(str(errmsg))\n","            success = False\n","print('Done')\n"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["##### Clean up - deletes recovered workspaces!!\n","<div style=\"display: flex; align-items: flex-end;\"><img style=\"float: left; margin-right: 10px;\" src=\"https://github.com/hurtn/images/blob/main/stop.png?raw=true\" width=\"50\"><span><h6>Only run the cell below if you are re-testing this process  or do not wish to keep the recovered workspaces in the secondary.<br>Keeping the cell frozen ensures it is not run when the Run all button is used.</span></div>"]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Please ensure you have run the workspaceutils command at the top of this notebook before running this cell to ensure all necessary imports and variables are loaded.\n","\n","print('Refreshing the workspaces metadata table...')\n","# Refresh the list of current workspaces\n","saveWorkspaceMeta()\n","\n","thisWsId = notebookutils.runtime.context['currentWorkspaceId'] #obtaining this so we don't accidently delete this workspace!\n","\n","delete_ws_sql = \"SELECT distinct ID,Type,Name FROM workspaces where Name like '%\"+p_secondary_ws_suffix+\"' and id != '\"+thisWsId+\"'\" \n","print('Deleting workspaces...')\n","# Get all workspaces created with the prefix set in the parameters at the top so that they can be deleted, except for this workspace of course!\n","df = spark.sql(delete_ws_sql).collect()\n","for i in df:\n","    #print(i['ID'])\n","    if i['Type'] == 'Workspace':\n","      workspaceId = i['ID']\n","      if p_logging_verbose:\n","        print(\"Deleting workspace \"+i['Name'] + '(' + i['ID'] + ')')\n","      response = client.delete(\"/v1/workspaces/\"+workspaceId)\n","      if p_logging_verbose and response.status_code ==200:\n","        print('Successfully deleted')\n","\n","print('Refreshing the workspaces metadata table after deleting recovered workspaces...')\n","# now refresh workspaces\n","saveWorkspaceMeta()\n","print('Done')\n"],"outputs":[]}]}